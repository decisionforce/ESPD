{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import random\n",
    "from IPython import embed\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as joindir\n",
    "from os import makedirs as mkdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import math\n",
    "def select_action(action_mean, action_logstd, fctr):\n",
    "    \"\"\"\n",
    "    given mean and std, sample an action from normal(mean, std)\n",
    "    also returns probability of the given chosen\n",
    "    \"\"\"\n",
    "    action_std = torch.exp(action_logstd)*fctr\n",
    "    action = torch.normal(action_mean, action_std)\n",
    "    return action\n",
    "def eval_policy_50(fctr_used):\n",
    "    reward_sum = 0\n",
    "    succ_game = 0\n",
    "    for display_i in range(50):\n",
    "        env.reset()\n",
    "        state = env.env._get_obs()\n",
    "        state = np.concatenate((state['observation'],state['desired_goal'])) # state_extended\n",
    "        episode = []\n",
    "        env_list = []\n",
    "        Succ_in_env = 0\n",
    "        for t in range(args.max_step_per_round):\n",
    "            network.eval()\n",
    "            action_mean, action_logstd, value = network(Tensor(state).unsqueeze(0))\n",
    "            action_mean = action_mean.detach()\n",
    "            action_logstd = action_logstd.detach()\n",
    "            value = value.detach()\n",
    "            action = select_action(action_mean, action_logstd, fctr_used)\n",
    "            action = torch.clamp(action,-1,1)\n",
    "            action = action.data.cpu().numpy()[0]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if _['is_success'] !=0:\n",
    "                Succ_in_env = 1\n",
    "                break\n",
    "\n",
    "            next_state = np.concatenate((next_state['observation'],next_state['desired_goal']))\n",
    "            reward_sum += reward\n",
    "            mask = 0 if done else 1\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        succ_game += Succ_in_env\n",
    "    return succ_game/50\n",
    "\n",
    "\n",
    "'''ablation study recorder'''\n",
    "Eval_different_sigma_recorder = []\n",
    "Tot_Ret_2 = []\n",
    "Tot_Ret_1 = []\n",
    "Tot_Ret_0 = []\n",
    "Acceptance_rate = []\n",
    "Shorter_And_Pass = []\n",
    "Shorter = []\n",
    "Pass_And_Shorter = []\n",
    "Pass = []\n",
    "Eval_list_0 = []\n",
    "Eval_list_0p1 = []\n",
    "Eval_list_0p2 = []\n",
    "Traj_num_recorder = []\n",
    "\n",
    "for repeat in range(5):\n",
    "    FACTOR = 1.0 # default\n",
    "    class args(object):\n",
    "        env_name = ''\n",
    "        seed = 1234 + repeat\n",
    "        num_episode = 200\n",
    "        batch_size = 2500\n",
    "        max_step_per_round = 200\n",
    "        gamma = 0.995\n",
    "        lamda = 0.97\n",
    "        log_num_episode = 1\n",
    "        num_epoch = 30\n",
    "        minibatch_size = 25\n",
    "        clip = 0.2\n",
    "        loss_coeff_value = 0.5\n",
    "        loss_coeff_entropy = 0.01\n",
    "        factor = FACTOR\n",
    "        lr_ppo =0*3e-4 # \n",
    "        lr_hid = 3e-4\n",
    "        future_p = 0.0 # param of HER\n",
    "        Horizon_max = 8 # Horizon K\n",
    "        reward_pos = 0. # positive reward value\n",
    "        factor = 1.0 # Exploration Factor sigma\n",
    "        num_parallel_run = 1\n",
    "        schedule_adam = 'linear'\n",
    "        schedule_clip = 'linear'\n",
    "        layer_norm = True\n",
    "        state_norm = False\n",
    "        advantage_norm = True\n",
    "        lossvalue_norm = True\n",
    "        replay_buffer_size_IER = 100000\n",
    "    \n",
    "    FACTOR_LIST = []\n",
    "    rwds = []\n",
    "    Succ_recorder = []\n",
    "    Horizon_list = [i+1 for i in range(args.Horizon_max)]\n",
    "    losses = [[] for i in range(len(Horizon_list)) ]\n",
    "    Transition = namedtuple('Transition', ('state', 'value', 'action', 'logproba', 'mask', 'next_state', 'reward'))\n",
    "    EPS = 1e-10\n",
    "    RESULT_DIR = 'result'\n",
    "    mkdir(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "    class ActorCritic(nn.Module):\n",
    "        def __init__(self, num_inputs, num_outputs, layer_norm=True):\n",
    "            super(ActorCritic, self).__init__()\n",
    "\n",
    "            self.actor_fc1 = nn.Linear(num_inputs, 64)\n",
    "            self.actor_fc2 = nn.Linear(64, 64)\n",
    "            self.actor_fc2_1 = nn.Linear(64,64)\n",
    "            self.actor_fc3 = nn.Linear(64, num_outputs)\n",
    "            self.actor_logstd = nn.Parameter(torch.zeros(1, num_outputs))\n",
    "\n",
    "            self.critic_fc1 = nn.Linear(num_inputs, 64)\n",
    "            self.critic_fc2 = nn.Linear(64, 64)\n",
    "            self.critic_fc3 = nn.Linear(64, 1)\n",
    "\n",
    "            if layer_norm:\n",
    "                self.layer_norm(self.actor_fc1, std=1.0)\n",
    "                self.layer_norm(self.actor_fc2, std=1.0)\n",
    "                self.layer_norm(self.actor_fc2_1, std=1.0)\n",
    "                self.layer_norm(self.actor_fc3, std=0.01)\n",
    "\n",
    "                self.layer_norm(self.critic_fc1, std=1.0)\n",
    "                self.layer_norm(self.critic_fc2, std=1.0)\n",
    "                self.layer_norm(self.critic_fc3, std=1.0)\n",
    "\n",
    "        @staticmethod\n",
    "        def layer_norm(layer, std=1.0, bias_const=0.0):\n",
    "            torch.nn.init.orthogonal_(layer.weight, std)\n",
    "            torch.nn.init.constant_(layer.bias, bias_const)\n",
    "\n",
    "        def forward(self, states):\n",
    "            \"\"\"\n",
    "            run policy network (actor) as well as value network (critic)\n",
    "            :param states: a Tensor2 represents states\n",
    "            :return: 3 Tensor2\n",
    "            \"\"\"\n",
    "            action_mean, action_logstd = self._forward_actor(states)\n",
    "            critic_value = self._forward_critic(states)\n",
    "            return action_mean, action_logstd, critic_value\n",
    "\n",
    "        def _forward_actor(self, states):\n",
    "            x = F.leaky_relu(self.actor_fc1(states))\n",
    "            x = F.leaky_relu(self.actor_fc2(x))\n",
    "            x = F.leaky_relu(self.actor_fc2_1(x))\n",
    "            action_mean = self.actor_fc3(x)\n",
    "            action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "            #embed()\n",
    "            assert action_logstd.mean() == 0\n",
    "            #print(action_logstd)\n",
    "            return action_mean, action_logstd\n",
    "\n",
    "        def _forward_critic(self, states):\n",
    "            x = torch.tanh(self.critic_fc1(states))\n",
    "            x = torch.tanh(self.critic_fc2(x))\n",
    "            critic_value = self.critic_fc3(x)\n",
    "            return critic_value\n",
    "\n",
    "        def select_action(self, action_mean, action_logstd, return_logproba=True, factor = 1.0):\n",
    "            \"\"\"\n",
    "            given mean and std, sample an action from normal(mean, std)\n",
    "            also returns probability of the given chosen\n",
    "            \"\"\"\n",
    "            action_std = torch.exp(action_logstd)*factor\n",
    "            action = torch.normal(action_mean, action_std)\n",
    "            if return_logproba:\n",
    "                logproba = self._normal_logproba(action, action_mean, action_logstd, action_std)\n",
    "            return action, logproba\n",
    "\n",
    "        @staticmethod\n",
    "        def _normal_logproba(x, mean, logstd, std=None):\n",
    "            if std is None:\n",
    "                std = torch.exp(logstd)\n",
    "\n",
    "            std_sq = std.pow(2)\n",
    "            logproba = - 0.5 * math.log(2 * math.pi) - logstd - (x - mean).pow(2) / (2 * std_sq)\n",
    "            return logproba.sum(1)\n",
    "\n",
    "        def get_logproba(self, states, actions):\n",
    "            \"\"\"\n",
    "            return probability of chosen the given actions under corresponding states of current network\n",
    "            :param states: Tensor\n",
    "            :param actions: Tensor\n",
    "            \"\"\"\n",
    "            action_mean, action_logstd = self._forward_actor(states)\n",
    "            logproba = self._normal_logproba(actions, action_mean, action_logstd)\n",
    "            return logproba\n",
    "\n",
    "        \n",
    "        \n",
    "    class ReplayBuffer_imitation(object):\n",
    "        def __init__(self, capacity):\n",
    "            self.buffer = {'1step':deque(maxlen=capacity)}\n",
    "            self.capacity = capacity\n",
    "        def push(self, state, action, step_num):\n",
    "            try:\n",
    "                self.buffer[step_num]\n",
    "            except:\n",
    "                self.buffer[step_num] = deque(maxlen=self.capacity)\n",
    "            self.buffer[step_num].append((state, action))\n",
    "\n",
    "\n",
    "        def sample(self, batch_size,step_num):\n",
    "            state, action= zip(*random.sample(self.buffer[step_num], batch_size))\n",
    "            return np.stack(state), action\n",
    "\n",
    "        def lenth(self,step_num):\n",
    "            try:\n",
    "                self.buffer[step_num]\n",
    "            except:\n",
    "                return 0\n",
    "            return len(self.buffer[step_num])\n",
    "\n",
    "        def __len__(self,step_num):\n",
    "            try:\n",
    "                self.buffer[step_num]\n",
    "            except:\n",
    "                return 0\n",
    "            return len(self.buffer[step_num])\n",
    "\n",
    "\n",
    "    env = gym.make('FetchPush-v1')  \n",
    "    num_inputs = env.observation_space.spaces['observation'].shape[0] +  env.observation_space.spaces['desired_goal'].shape[0] # extended state\n",
    "    num_actions = env.action_space.shape[0]\n",
    "    network = ActorCritic(num_inputs, num_actions, layer_norm=args.layer_norm)\n",
    "\n",
    "    '''joint train'''\n",
    "    model_imitation = network\n",
    "\n",
    "\n",
    "    def ppo(args):\n",
    "        def compute_cross_ent_error(batch_size,step_num):\n",
    "            if ier_buffer.lenth(step_num)==0:\n",
    "                return None\n",
    "            if batch_size>ier_buffer.lenth(step_num):\n",
    "                return None\n",
    "            state, action= ier_buffer.sample(batch_size,step_num)\n",
    "            state          = torch.FloatTensor(state)#.to(device)\n",
    "            action_target  = torch.FloatTensor(action)#.to(device)\n",
    "            action_pred    = model_imitation(state)[0]\n",
    "\n",
    "            loss_func = nn.MSELoss()\n",
    "            loss = loss_func(action_pred,action_target)\n",
    "            optimizer_imitation.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_imitation.step()\n",
    "            return loss\n",
    "        def test_isvalid_multistep(step_lenth, state_start, environment_start,env):\n",
    "            env_tim = env\n",
    "            env_tim.sim.set_state(environment_start)\n",
    "            env_tim.sim.forward()\n",
    "            state_tim = deepcopy(state_start)\n",
    "            for step_i in range(step_lenth):\n",
    "                action_tim_mean, action_tim_logstd, value_tim = network(Tensor(state_tim).unsqueeze(0))\n",
    "                action_tim = action_tim_mean.data.numpy()[0]\n",
    "                next_state_tim, reward, done, _ = env_tim.step(action_tim)\n",
    "                next_state_tim = np.concatenate((next_state_tim['observation'],next_state_tim['desired_goal']))\n",
    "\n",
    "                next_state_tim[-3:] = deepcopy(state_tim[-3:])\n",
    "\n",
    "                rwd_sim = env_tim.compute_reward(next_state_tim[3:6],next_state_tim[-3:],{'is_success': 0.0})\n",
    "                if rwd_sim == 0:\n",
    "                    if step_i <= step_lenth-1:\n",
    "                        return 1 # should not learn\n",
    "                    else:\n",
    "                        return 0 # can learn\n",
    "                state_tim = next_state_tim\n",
    "            return 2 # should learn\n",
    "            \n",
    "        FACTOR = args.factor\n",
    "        env = gym.make(args.env_name)\n",
    "        num_inputs = env.observation_space.spaces['observation'].shape[0]+ env.observation_space.spaces['desired_goal'].shape[0] # extended state\n",
    "        num_actions = env.action_space.shape[0]\n",
    "\n",
    "        env.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "\n",
    "        optimizer = opt.RMSprop(network.parameters(), lr=args.lr_ppo)\n",
    "        optimizer_imitation = opt.RMSprop(model_imitation.parameters(),lr = args.lr_hid)\n",
    "\n",
    "\n",
    "        reward_record = []\n",
    "        global_steps = 0\n",
    "\n",
    "        lr_now = args.lr_ppo\n",
    "        clip_now = args.clip\n",
    "        ier_buffer = ReplayBuffer_imitation(args.replay_buffer_size_IER)\n",
    "        \n",
    "        for i_episode in range(args.num_episode):\n",
    "            episodic_pass_test_num = 0 \n",
    "            num_steps = 0\n",
    "            reward_list = []\n",
    "            len_list = []\n",
    "            Succ_num = 0\n",
    "\n",
    "            game_num = 0\n",
    "            succ_game = 0\n",
    "            \n",
    "            '''calculate acceptance rate'''\n",
    "            Ret_2 = [0*_ for _ in range(len(Horizon_list))]\n",
    "            Ret_1 = [0*_ for _ in range(len(Horizon_list))]\n",
    "            Ret_0 = [0*_ for _ in range(len(Horizon_list))]\n",
    "            \n",
    "            pass_1 = 0\n",
    "            pass_2 = 0\n",
    "            \n",
    "            while num_steps < args.batch_size:\n",
    "                \n",
    "                \"\"\"interaction\"\"\"\n",
    "                \n",
    "                state = env.reset()\n",
    "                game_num +=1\n",
    "                state = np.concatenate((state['observation'],state['desired_goal'])) # state_extended\n",
    "\n",
    "                reward_sum = 0\n",
    "                episode = []\n",
    "                env_list = []\n",
    "                Succ_in_env = 0\n",
    "                for t in range(args.max_step_per_round):\n",
    "                    action_mean, action_logstd, value = network(Tensor(state).unsqueeze(0))\n",
    "                    action, logproba = network.select_action(action_mean, action_logstd,factor = FACTOR)\n",
    "                    \n",
    "                    action = torch.clamp(action,-1,1)\n",
    "                    action = action.data.numpy()[0]\n",
    "                    logproba = logproba.data.numpy()[0]\n",
    "                    \n",
    "                    if len(Horizon_list) >= 2:\n",
    "                        state_temp = env.env.sim.get_state()\n",
    "                        env_list.append(state_temp)\n",
    "                    \n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "                    if reward ==0:\n",
    "                        Succ_in_env = 1\n",
    "                        reward = args.reward_pos \n",
    "                        Succ_num+=1\n",
    "                    next_state = np.concatenate((next_state['observation'],next_state['desired_goal']))\n",
    "\n",
    "                    reward_sum += reward\n",
    "                    mask = 0 if done else 1\n",
    "\n",
    "                    episode.append((state, value, action, logproba, mask, next_state, reward))\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                    state = next_state\n",
    "                succ_game += Succ_in_env\n",
    "                \n",
    "                '''interaction end, start to learn with Memory'''\n",
    "                \n",
    "                for ind,(state, value, action, logproba, mask, next_state, reward) in enumerate(episode):\n",
    "                    if len(Horizon_list)>=2:\n",
    "                        assert len(env_list) == len(episode)\n",
    "                    '''ESPD: supervised learning part'''\n",
    "                    for t_ in Horizon_list:\n",
    "                        try:\n",
    "                            episode[t_+ind]\n",
    "                        except:\n",
    "                            break\n",
    "                        \n",
    "                        target_state_ = deepcopy(episode[t_+ind][-7])\n",
    "                        state_ = deepcopy(state)\n",
    "                        state_[-3:] = deepcopy(target_state_[3:6])\n",
    "                        rwd_temp3 = np.linalg.norm(target_state_[3:6]-state_[3:6])\n",
    "                        if rwd_temp3 > 0.05:\n",
    "                            ret_tim = test_isvalid_multistep(t_, state_, env_list[ind],env)\n",
    "                            if ret_tim==2:\n",
    "                                pass_1+=1\n",
    "                                \n",
    "                                ier_buffer.push(state_,action,'1step')\n",
    "                                episodic_pass_test_num += 1\n",
    "                                Ret_2[t_-1] +=1\n",
    "                                \n",
    "                            elif ret_tim == 1:\n",
    "                                Ret_1[t_-1] +=1\n",
    "                            else:\n",
    "                                Ret_0[t_-1] +=1\n",
    "                \n",
    "                num_steps += (t + 1)\n",
    "                global_steps += (t + 1)\n",
    "                reward_list.append(reward_sum)\n",
    "                len_list.append(t + 1)\n",
    "                Winrate = 1.0*succ_game/game_num\n",
    "                Succ_recorder.append(Winrate)\n",
    "            \n",
    "            Traj_num_recorder.append(Ret_2)\n",
    "            print('Return This Episode:',Ret_0,Ret_1,Ret_2)\n",
    "            Acceptance_rate.append([round((Ret_2[_]/(Ret_2[_] + Ret_1[_] + Ret_0[_] + 1e-6))*100.0)/100.0 for _ in range(len(Ret_2))])\n",
    "            Tot_Ret_2.append(Ret_2)\n",
    "            Tot_Ret_1.append(Ret_1)\n",
    "            Tot_Ret_0.append(Ret_0)\n",
    "            \n",
    "            reward_record.append({\n",
    "                'episode': i_episode, \n",
    "                'steps': global_steps, \n",
    "                'meanepreward': np.mean(reward_list), \n",
    "                'meaneplen': np.mean(len_list)})\n",
    "\n",
    "            rwds.extend(reward_list)\n",
    "            batch_size = episodic_pass_test_num\n",
    "\n",
    "            SR = 1.0*Succ_num/num_steps\n",
    "            \n",
    "            for i_epoch in range(int(args.num_epoch * batch_size / args.minibatch_size)):\n",
    "                '''learning'''\n",
    "                for h in [1]:\n",
    "                    los_lst = []\n",
    "                    flag = 0\n",
    "                    loss1 = compute_cross_ent_error(args.minibatch_size,str(h)+'step')\n",
    "                    if loss1 is not None:\n",
    "                        flag = 1\n",
    "                        losses[h-1].append(loss1.item())\n",
    "                        los_lst.append('loss{}'.format(h))\n",
    "                        \n",
    "            print('ier lenth',ier_buffer.lenth('1step'),ier_buffer.lenth('2step'),ier_buffer.lenth('3step'),ier_buffer.lenth('4step'),ier_buffer.lenth('5step'),ier_buffer.lenth('6step'),ier_buffer.lenth('7step'))\n",
    "            if True:\n",
    "                FACTOR = args.factor\n",
    "                \n",
    "            print('factor now is ',FACTOR)\n",
    "            eval_0_temp = eval_policy_50(0.0)\n",
    "            eval_0p1_temp = eval_policy_50(0.1)\n",
    "            eval_0p2_temp = eval_policy_50(0.2)\n",
    "            Eval_list_0.append(eval_0_temp)\n",
    "            Eval_list_0p1.append(eval_0p1_temp)\n",
    "            Eval_list_0p2.append(eval_0p2_temp)\n",
    "            print('Eval_sr:',eval_0_temp,eval_0p1_temp,eval_0p2_temp)\n",
    "            print('Traj length in this episode',Ret_2)\n",
    "            \n",
    "            if args.schedule_clip == 'linear':\n",
    "                ep_ratio = 1 - (i_episode / args.num_episode)\n",
    "                clip_now = args.clip * ep_ratio\n",
    "\n",
    "            if args.schedule_adam == 'linear':\n",
    "                ep_ratio = 1 - (i_episode / args.num_episode)\n",
    "                lr_now = args.lr_ppo * ep_ratio\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] = lr_now\n",
    "\n",
    "            if i_episode % args.log_num_episode == 0:\n",
    "                print('Finished episode: {} Reward: {:.4f} SuccessRate{:.4f} WinRate{:.4f}' \\\n",
    "                    .format(i_episode, reward_record[-1]['meanepreward'],SR,Winrate))\n",
    "                print('-----------------')\n",
    "        return reward_record\n",
    "\n",
    "    def test(args):\n",
    "        record_dfs = []\n",
    "        for i in range(args.num_parallel_run):\n",
    "            args.seed += 1\n",
    "            reward_record = pd.DataFrame(ppo(args))\n",
    "            reward_record['#parallel_run'] = i\n",
    "            record_dfs.append(reward_record)\n",
    "        record_dfs = pd.concat(record_dfs, axis=0)\n",
    "        record_dfs.to_csv(joindir(RESULT_DIR, 'ppo-record-{}.csv'.format(args.env_name)))\n",
    "   \n",
    "\n",
    "    for envname in ['FetchPush-v1']:\n",
    "        args.env_name = envname\n",
    "        test(args)\n",
    "        rwds_HER_HID= deepcopy(rwds)\n",
    "        Succ_recorder_HER_HID= deepcopy(Succ_recorder)\n",
    "        np.save('results/Push_Factor{0}_rewards_repeat{1}'.format(args.factor,repeat),rwds_HER_HID)\n",
    "        np.save('results/Push_Factor{0}_SR_repeat{1}'.format(args.factor,repeat),Succ_recorder_HER_HID)\n",
    "        np.save('results/Push_Factor{0}_Eval_list_0_repeat{1}'.format(args.factor,repeat),Eval_list_0)\n",
    "        np.save('results/Push_Factor{0}_Eval_list_0p1_repeat{1}'.format(args.factor,repeat),Eval_list_0p1)\n",
    "        np.save('results/Push_Factor{0}_Eval_list_0p2_repeat{1}'.format(args.factor,repeat),Eval_list_0p2)\n",
    "        np.save('results/Push_Factor{0}_Tot_Ret_0_repeat{1}'.format(args.factor,repeat),Tot_Ret_0)\n",
    "        np.save('results/Push_Factor{0}_Tot_Ret_1_repeat{1}'.format(args.factor,repeat),Tot_Ret_1)\n",
    "        np.save('results/Push_Factor{0}_Tot_Ret_2_repeat{1}'.format(args.factor,repeat),Tot_Ret_2)\n",
    "        np.save('results/Push_Factor{0}_Tot_Ret_2_repeat{1}'.format(args.factor,repeat),Acceptance_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
